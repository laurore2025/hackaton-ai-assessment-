
# -*- coding: utf-8 -*-

# === D√©sactiver compl√®tement Weights & Biases (W&B) ===
import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "dryrun"  # mode inactif complet
os.environ["WANDB_SILENT"] = "true"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "true"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
print("‚úÖ WandB d√©sactiv√© avec succ√®s.")

# === Cellule 0 : V√©rifier GPU (ex√©cuter en premier) ===
import torch
print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    try:
        print("Device:", torch.cuda.get_device_name(0))
    except:
        pass
else:
    print("Pas de GPU ‚Äî l'entra√Ænement sera lent sur CPU.")

# === Cellule 1 : Installer d√©pendances (Colab) ===
!pip install -q transformers datasets evaluate peft bitsandbytes scikit-learn sentencepiece gradio
# bitsandbytes est optionnel, utile pour quantization 8-bit sur certains GPU
print("Install termin√©")

# === Cellule 2 : Imports & configuration ===
import pandas as pd
import numpy as np
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import evaluate
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding
from peft import get_peft_model, LoraConfig, TaskType, PeftModel
import matplotlib.pyplot as plt
import gradio as gr
print("Imports OK")

# === Cellule 3 : Monter Google Drive pour sauvegarder ===
from google.colab import drive
drive.mount('/content/drive', force_remount=False)
SAVE_DIR = "/content/drive/MyDrive/sti_project"
os.makedirs(SAVE_DIR, exist_ok=True)
print("Sauvegardes dans :", SAVE_DIR)

# === Cellule 4 : Charger ou cr√©er le dataset ===
create_fake = True  # ‚Üê Mettre False pour charger ton CSV r√©el

if create_fake:
    # Dataset factice pour test
    texts = [
        "Douleur √† la miction et √©coulement vaginal anormal",
        "Aucun sympt√¥me, consultation de routine",
        "Br√ªlures et pertes malodorantes",
        "Douleur pelvienne sans autres sympt√¥mes",
        "√âcoulement purulent, douleur, fi√®vre",
        "Pas de plainte, test n√©gatif",
        "D√©mangeaisons et irritation g√©nitale",
        "Sympt√¥mes grippaux avec √©ruption cutan√©e",
        "Examen m√©dical normal",
        "Contr√¥le post-traitement sans sympt√¥mes"
    ] * 50  # 500 √©chantillons
    labels = [1, 0, 1, 0, 1, 0, 1, 1, 0, 0] * 50
    df = pd.DataFrame({"symptoms_text": texts, "has_sti": labels})
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    print("Dataset factice cr√©√©, taille:", len(df))
    print("R√©partition des labels:")
    print(df["has_sti"].value_counts(normalize=True))
else:
    # Charger ton CSV r√©el
    data_path = "/content/drive/MyDrive/sti_project/stis.csv"
    df = pd.read_csv(data_path)
    print("CSV charg√© :", data_path)

display(df.head())

# === Cellule 5 : Anonymisation et pr√©paration des donn√©es ===
# Supprimer les colonnes d'identification
to_drop = ["patient_id", "name", "address", "phone", "email", "ssn"]
for c in to_drop:
    if c in df.columns:
        df = df.drop(columns=c)

# G√©rer la date de naissance si pr√©sente
if "date_of_birth" in df.columns:
    df["age_years"] = pd.to_datetime("today").year - pd.to_datetime(df["date_of_birth"]).dt.year
    df["age_group"] = pd.cut(df["age_years"], bins=[0, 18, 25, 35, 50, 120],
                            labels=["<18", "18-25", "26-35", "36-50", "50+"])
    df = df.drop(columns=["date_of_birth", "age_years"])

# Cr√©er la colonne has_sti si elle n'existe pas
if "has_sti" not in df.columns:
    if "diagnosis" in df.columns:
        sti_set = {"chlamydia", "gonorrhea", "syphilis", "trichomoniasis", "herpes"}
        df["has_sti"] = df["diagnosis"].astype(str).str.lower().apply(
            lambda x: 1 if any(sti in x.lower() for sti in sti_set) else 0
        )
        print("Colonne has_sti cr√©√©e depuis diagnosis.")
    else:
        raise ValueError("Colonne 'has_sti' manquante et impossible √† cr√©er depuis 'diagnosis'")

print("Colonnes disponibles :", df.columns.tolist())
display(df.head())

# === Cellule 6 : Split train / valid / test ===
text_col = "symptoms_text"

if text_col not in df.columns:
    raise ValueError(f"Colonne texte '{text_col}' introuvable.")

# Garder uniquement les colonnes utiles
keep_cols = [text_col, "has_sti"] + [c for c in ["age_group", "gender"] if c in df.columns]
df = df[keep_cols].dropna(subset=[text_col, "has_sti"]).reset_index(drop=True)

# Split stratifi√©
train_df, test_df = train_test_split(df, test_size=0.15, stratify=df["has_sti"], random_state=42)
train_df, valid_df = train_test_split(train_df, test_size=0.15, stratify=train_df["has_sti"], random_state=42)

# Cr√©er le DatasetDict
ds = DatasetDict({
    "train": Dataset.from_pandas(train_df.reset_index(drop=True)),
    "validation": Dataset.from_pandas(valid_df.reset_index(drop=True)),
    "test": Dataset.from_pandas(test_df.reset_index(drop=True))
})
print("Tailles splits:", {k: len(ds[k]) for k in ds})

# === Cellule 7 : Charger le tokenizer ===
model_name = "camembert-base"
num_labels = 2

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
print("Tokeniser charg√©:", model_name, "vocab_size:", tokenizer.vocab_size)

# === Cellule 8 : Tokenization avec m√©tadonn√©es ===
max_len = 256

def preprocess_and_label(batch):
    texts = []
    for i in range(len(batch[text_col])):
        t = str(batch[text_col][i])
        meta = []
        if "age_group" in batch and batch["age_group"][i] is not None:
            meta.append(f"AGE:{batch['age_group'][i]}")
        if "gender" in batch and batch["gender"][i] is not None:
            meta.append(f"GENDER:{batch['gender'][i]}")
        prefix = " ".join(meta)
        if prefix:
            texts.append(prefix + " | " + t)
        else:
            texts.append(t)
    
    enc = tokenizer(texts, truncation=True, padding="max_length", max_length=max_len)
    enc["labels"] = [int(x) for x in batch["has_sti"]]
    return enc

tokenized = ds.map(preprocess_and_label, batched=True)
print("Tokenization + labels OK")
print("Colonnes du dataset tokenis√© (train):", tokenized["train"].column_names)

# === Cellule 9 : Configuration LoRA et chargement du mod√®le ===
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["query", "key", "value", "dense"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_CLS
)
model = get_peft_model(model, lora_config)
print("Mod√®le + LoRA pr√™t ‚Äî param√®tres LoRA ajout√©s.")

# Afficher le nombre de param√®tres entra√Ænables
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Param√®tres entra√Ænables: {trainable_params:,} / {total_params:,} ({trainable_params/total_params:.2%})")

# === Cellule 10 : Configuration de l'entra√Ænement (CORRIG√âE) ===
# Version compatible avec les nouvelles versions de transformers
training_args = TrainingArguments(
    output_dir="/content/outputs/sti_camembert_lora",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    num_train_epochs=4,
    eval_strategy="epoch",  # CORRECTION: 'evaluation_strategy' -> 'eval_strategy'
    save_strategy="epoch",
    logging_steps=50,
    fp16=torch.cuda.is_available(),
    load_best_model_at_end=True,
    metric_for_best_model="eval_f1",
    greater_is_better=True,
    save_total_limit=3,
    report_to="none"  # D√©sactive compl√®tement W&B
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# M√©triques d'√©valuation
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": float(accuracy.compute(predictions=preds, references=labels)["accuracy"]),
        "f1": float(f1.compute(predictions=preds, references=labels, average="weighted")["f1"])
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)
print("Trainer pr√™t")

# === Cellule 11 : Entra√Ænement du mod√®le ===
print("D√©but de l'entra√Ænement...")
train_result = trainer.train()
print("Entra√Ænement termin√©.")

# Sauvegarder le mod√®le
OUT = os.path.join(SAVE_DIR, "sti_camembert_lora")
os.makedirs(OUT, exist_ok=True)
trainer.save_model(OUT)
tokenizer.save_pretrained(OUT)
model.save_pretrained(OUT)  # Sauvegarde sp√©cifique PEFT
print(f"Mod√®le sauvegard√© dans : {OUT}")

# √âvaluation finale sur validation
eval_metrics = trainer.evaluate(eval_dataset=tokenized["validation"])
print("Metrics validation finale:", eval_metrics)

# === Cellule 12 : √âvaluation sur le jeu de test ===
print("√âvaluation sur le jeu de test...")
preds_output = trainer.predict(tokenized["test"])
logits = preds_output.predictions

if logits.ndim == 2:
    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()
    y_pred = np.argmax(logits, axis=-1)
    y_proba_pos = probs[:, 1]
else:
    y_pred = (logits > 0).astype(int)
    y_proba_pos = y_pred.astype(float)

y_true = np.array(tokenized["test"]["labels"])

print("\n=== Classification report (test) ===")
print(classification_report(y_true, y_pred, digits=4))

print("\n=== Matrice de confusion ===")
cm = confusion_matrix(y_true, y_pred)
print(cm)

# M√©triques d√©taill√©es
metrics_test = {
    "accuracy": float((y_pred == y_true).mean()),
    "f1": float(f1.compute(predictions=y_pred, references=y_true, average="weighted")["f1"])
}
try:
    metrics_test["roc_auc"] = float(roc_auc_score(y_true, y_proba_pos))
except Exception as e:
    metrics_test["roc_auc"] = None
    print(f"ROC AUC non calculable: {e}")

print("\nMetrics r√©sum√© test:", metrics_test)

# === Cellule 13 : Visualisations ===
# Matrice de confusion
plt.figure(figsize=(6, 5))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title("Matrice de confusion - Test")
plt.colorbar()
classes = ["Sans IST (0)", "Avec IST (1)"]
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)
plt.xlabel("Pr√©diction")
plt.ylabel("Vrai label")

# Ajouter les valeurs dans les cases
thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, format(cm[i, j], 'd'),
                ha="center", va="center",
                color="white" if cm[i, j] > thresh else "black")
plt.tight_layout()
plt.show()

# Courbe ROC si binaire
if len(np.unique(y_true)) == 2 and metrics_test["roc_auc"] is not None:
    fpr, tpr, _ = roc_curve(y_true, y_proba_pos)
    auc = metrics_test["roc_auc"]
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Courbe ROC (AUC = {auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Taux de Faux Positifs')
    plt.ylabel('Taux de Vrais Positifs')
    plt.title('Courbe ROC - Test')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.show()
else:
    print("Courbe ROC non trac√©e (probl√®me binaire requis)")

# === Cellule 14 : Sauvegarde des pr√©dictions ===
# Reconstruire le DataFrame test avec pr√©dictions
test_pd = test_df.reset_index(drop=True).copy()
test_pd["pred"] = y_pred
test_pd["proba_pos"] = y_proba_pos
test_pd["proba_neg"] = 1 - y_proba_pos
test_pd["correct"] = test_pd["has_sti"] == test_pd["pred"]

# Sauvegarder toutes les pr√©dictions
preds_fp = os.path.join(OUT, "test_predictions_full.csv")
test_pd.to_csv(preds_fp, index=False)
print("Pr√©dictions compl√®tes sauvegard√©es :", preds_fp)

# Exemples mal class√©s
bad_predictions = test_pd[~test_pd["correct"]]
bad_fp = os.path.join(OUT, "mauvaise_pred_examples.csv")
bad_predictions.to_csv(bad_fp, index=False)
print(f"Exemples mal class√©s ({len(bad_predictions)}) sauvegard√©s :", bad_fp)

# Statistiques des mauvaises pr√©dictions
if len(bad_predictions) > 0:
    print("\nAnalyse des mauvaises pr√©dictions:")
    print(bad_predictions["has_sti"].value_counts().rename("Mauvaises pr√©dictions par classe"))

# === Cellule 15 : Interface Gradio ===
print("Pr√©paration de l'interface Gradio...")

# Recharger le mod√®le pour l'inf√©rence (meilleure pratique)
try:
    tokenizer_inf = AutoTokenizer.from_pretrained(OUT)
    base_model_inf = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
    model_inf = PeftModel.from_pretrained(base_model_inf, OUT)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_inf.eval()
    model_inf.to(device)
    print(f"Mod√®le recharg√© pour inf√©rence sur {device}")
    
except Exception as e:
    print(f"Erreur rechargement mod√®le: {e}")
    print("Utilisation du mod√®le existant...")
    model_inf = model
    tokenizer_inf = tokenizer
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_inf.to(device)
    model_inf.eval()

def predict_sti(text, age_group=None, gender=None):
    """
    Pr√©dit si le texte d√©crit des sympt√¥mes d'IST
    """
    if not text or text.strip() == "":
        return "‚ùå Veuillez entrer une description des sympt√¥mes", 0.0, 0.0
    
    try:
        # Construire le texte avec m√©tadonn√©es optionnelles
        meta_parts = []
        if age_group and age_group != "Non sp√©cifi√©":
            meta_parts.append(f"AGE:{age_group}")
        if gender and gender != "Non sp√©cifi√©":
            meta_parts.append(f"GENDER:{gender}")
        
        if meta_parts:
            full_text = " ".join(meta_parts) + " | " + text.strip()
        else:
            full_text = text.strip()
        
        # Tokenization
        inputs = tokenizer_inf(full_text, return_tensors="pt", truncation=True,
                             padding=True, max_length=max_len)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Pr√©diction
        with torch.no_grad():
            outputs = model_inf(**inputs)
            logits = outputs.logits
        
        # Conversion en probabilit√©s
        probs = torch.softmax(logits, dim=-1)
        prob_positive = probs[0][1].item()
        prob_negative = probs[0][0].item()
        
        # Interpr√©tation
        if prob_positive > 0.7:
            prediction = "üü° IST probable - Consultation recommand√©e"
            confidence = prob_positive
        elif prob_positive > 0.3:
            prediction = "üü† Suspicion d'IST - Consultation conseill√©e"
            confidence = prob_positive
        else:
            prediction = "üü¢ Aucun signe d'IST d√©tect√©"
            confidence = prob_negative
        
        return prediction, prob_positive, prob_negative
        
    except Exception as e:
        return f"‚ùå Erreur lors de la pr√©diction: {str(e)}", 0.0, 0.0

# Test de la fonction
print("Test de la fonction de pr√©diction...")
test_text = "Douleur √† la miction et √©coulement vaginal"
result, prob_pos, prob_neg = predict_sti(test_text)
print(f"Test: '{test_text}' -> {result} (prob IST: {prob_pos:.3f})")

# Cr√©ation de l'interface Gradio
age_options = ["Non sp√©cifi√©", "<18", "18-25", "26-35", "36-50", "50+"]
gender_options = ["Non sp√©cifi√©", "F", "M", "Autre"]

with gr.Blocks(title="D√©tecteur d'IST - Analyse de sympt√¥mes", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üîç D√©tecteur d'IST - Analyse de sympt√¥mes
    Cet outil analyse la description de sympt√¥mes pour d√©tecter des signes d'Infections Sexuellement Transmissibles (IST).
    
    **‚ö†Ô∏è Attention**: Ceci est un outil d'aide √† la d√©cision, pas un diagnostic m√©dical. Consultez toujours un professionnel de sant√©.
    """)
    
    with gr.Row():
        with gr.Column():
            symptoms_input = gr.Textbox(
                label="Description des sympt√¥mes",
                placeholder="Ex: Douleur √† la miction, √©coulement anormal, br√ªlures...",
                lines=3,
                max_lines=6
            )
            
            age_dropdown = gr.Dropdown(
                label="Groupe d'√¢ge (optionnel)",
                choices=age_options,
                value="Non sp√©cifi√©"
            )
            
            gender_dropdown = gr.Dropdown(
                label="Genre (optionnel)",
                choices=gender_options,
                value="Non sp√©cifi√©"
            )
            
            analyze_btn = gr.Button("Analyser les sympt√¥mes", variant="primary")
        
        with gr.Column():
            prediction_output = gr.Textbox(
                label="R√©sultat de l'analyse",
                interactive=False,
                lines=2
            )
            
            with gr.Row():
                prob_positive = gr.Number(
                    label="Probabilit√© IST",
                    interactive=False,
                    precision=3
                )
                prob_negative = gr.Number(
                    label="Probabilit√© absence IST",
                    interactive=False,
                    precision=3
                )
    
    # Exemples rapides
    gr.Markdown("### Exemples rapides:")
    examples = gr.Examples(
        examples=[
            ["Douleur √† la miction et √©coulement vaginal anormal", "26-35", "F"],
            ["Aucun sympt√¥me, consultation de routine", "Non sp√©cifi√©", "Non sp√©cifi√©"],
            ["Br√ªlures et pertes malodorantes", "18-25", "F"],
            ["D√©mangeaisons et irritation g√©nitale", "Non sp√©cifi√©", "M"]
        ],
        inputs=[symptoms_input, age_dropdown, gender_dropdown],
        outputs=[prediction_output, prob_positive, prob_negative],
        fn=predict_sti,
        cache_examples=False
    )
    
    # Liaison du bouton
    analyze_btn.click(
        fn=predict_sti,
        inputs=[symptoms_input, age_dropdown, gender_dropdown],
        outputs=[prediction_output, prob_positive, prob_negative]
    )
    
    # Disclaimer
    gr.Markdown("""
    ---
    **Disclaimer m√©dical**: 
    - Cet outil utilise l'IA pour analyser les sympt√¥mes d√©crits
    - Il ne remplace pas une consultation m√©dicale professionnelle
    - En cas de sympt√¥mes, consultez un m√©decin ou un centre de sant√©
    - Les r√©sultats sont fournis √† titre informatif seulement
    """)

print("Interface Gradio cr√©√©e avec succ√®s!")
print("Pour lancer l'interface, ex√©cutez: demo.launch(share=True)")

# Lancer l'interface
try:
    demo.launch(share=True, debug=True)
except Exception as e:
    print(f"Erreur lancement Gradio: {e}")
    print("Tentative sans partage...")
    demo.launch(share=False, debug=True)

print("‚úÖ Code ex√©cut√© avec succ√®s!")
